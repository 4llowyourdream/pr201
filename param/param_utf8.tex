\documentclass[pdftex,12pt,a4paper]{article}

\input{/home/boris/Dropbox/Public/tex_general/title_bor_utf8}

%\usepackage{showkeys} % показывать метки

\input{/home/boris/Dropbox/Public/tex_general/prob_and_sol_utf8}

%\title{Задачи по элементарной теории вероятностей и матстатистике}
%\author{Составитель: Борис Демешев, boris.demeshev@gmail.com}
%\date{\today}

\begin{document}


\parindent=0 pt % Отступ равен 0

Коротко о некоторых параметрических тестах. (ver 01.08.06). \\
Текст можно скачать на www.xion.ru (учеба-2 курс-теория
вероятностей). \\
Вопросы/комментарии/предложения/найденные ошибки можно смело
отправлять на
roah@yandex.ru (Борису Демешеву) \\


\textbf{Quote} \\
Во избежание несчастных случаев торпеды хранить так, чтобы верхняя
их часть находилась внизу, а нижняя наверху. Дабы персонал не
путал верх с низом, на верхней части торпеды сделать надпись
<<верх>>
\begin{flushright}
\emph{из инструкции} \\
\end{flushright}


\textbf{Обозначения} \\

$\bar{X}=\frac{X_{1}+...+X_{n}}{n}$ - несмещенная оценка математического ожидания \\
$\hat{\sigma}^{2}=\frac{\sum_{i}(X_{i}-\bar{X})^{2}}{n-1}$ - несмещенная оценка дисперсии \\
$\hat{v}^{2}=\frac{\sum_{i}(X_{i}-\bar{X})^{2}}{n}$ - оценка
дисперсии,
при больших $n$ слабо отличается от $\hat{\sigma}^{2}$ \\

\textbf{Случай 0-1}\\
Часто рассматривается случай, когда $X_{i}$ принимают только
значения 0 и 1. Значение 1 с вероятностью $p$, значение 0 с
вероятностью $q=1-p$. \\
В этом случае:\\
1. $E(X_{i})=p$, $Var(X_{i})=pq=p(1-p)$. \\
2. Вместо $\bar{X}$ часто используется обозначение $\hat{p}$ \\
3. $\hat{v}^{2}=\hat{p}(1-\hat{p})$ \\
Доказательство 3: \\
$\sum(X_{i}-\bar{X})^{2}=\sum(X_{i}^{2}+\bar{X}^{2}-2X_{i}\bar{X})= \\
= \sum X_{i}+n\bar{X}^{2}-2\bar{X}\sum
X_{i}=n\bar{X}+n\bar{X}^{2}-2n\bar{X}^{2}=n\bar{X}-n\bar{X}^{2}=n\hat{p}(1-\hat{p})$
\\

\textbf{Definitions} \\
Определение 1. \\
Распределение случайной величины $K$ называется $\chi^{2}$
распределением с $n$ степенями свободы, если величину можно
представить в виде $K=Z_{1}^{2}+...+Z_{n}^{2}$, где $Z_{i}$ iid,
$N(0;1)$ \\
Т.е. есть $\chi^{2}$-распределение номер 1, есть
$\chi^{2}$-распределение номер 2 и т.д.

Определение 2. \\
Распределение случайной величины $T$ называется $t$-распределением
с $n$ степенями свободы, если величину можно представить в виде
$T=\frac{Z}{\sqrt{\frac{K}{n}}}$, где $Z \sim N(0;1)$, $K\sim
\chi_{n}^{2}$, $Z$ и $K$ независимы \\

Определение 3. \\
Распределение случайной величины $F$ называется $F$-распределением
с $n$, $k$ степенями свободы, если величину можно представить в
виде $F=\frac{X/n}{Y/k}$, где $X\sim \chi_{n}^{2}$ и $Y\sim
\chi_{k}^{2}$ и $X$ и $Y$ независимы \\

\textbf{Одна выборка} \\

Асимптотический результат. \\
Если: \\
1. $X_{i}$ - независимы, одинаково распределены, \\
2. $E(X_{i})=\mu$, $Var(X_{i})=\sigma^{2}$, \\
То: \\
1. $Z=\frac{\bar{X}_{n}-\mu}{\sqrt{\frac{\sigma^{2}}{n}}}$ имеет
асимптотически  (т.е. при $n\to\infty$) нормальное распределение $N(0;1)$ \\
2. $Z=\frac{\bar{X}_{n}-\mu}{\sqrt{\frac{\hat{\sigma}^{2}}{n}}}$
имеет
асимптотически  (т.е. при $n\to\infty$) нормальное распределение $N(0;1)$ \\
Примечание: \\
В пункте 2 вместо $\hat{\sigma}^{2}$ можно взять любую другую
состоятельную оценку дисперсии, например $\hat{v}^{2}$ \\

Точный результат. \\
Добавив дополнительное условие нормальности отдельных $X_{i}$
получаем: \\
Если: \\
1. $X_{i}$ - независимы, одинаково распределены, \\
2. $E(X_{i})=\mu$, $Var(X_{i})=\sigma^{2}$, \\
3. $X_{i}$ - нормально распределены, \\
То: \\
1. $Z=\frac{\bar{X}_{n}-\mu}{\sqrt{\frac{\sigma^{2}}{n}}}$ имеет
 нормальное распределение $N(0;1)$ \\
2. $Z=\frac{\bar{X}_{n}-\mu}{\sqrt{\frac{\hat{\sigma}^{2}}{n}}}$
имеет
 $t$-распределение с $n-1$ степенью свободы \\
3. $Q=\frac{\hat{(n-1)\sigma}^{2}}{\sigma^{2}}$ имеет $\chi^{2}$
распределение с $n-1$ степенью свободы. \\

Частный случай асимптотического результата. \\
Отдельно рассмотрим случай 0-1: \\
Если: \\
1. $X_{i}$ - независимы, одинаково распределены, \\
2. $X_{i}$ принимает значение 1 с вероятностью $p$ и значение 0 c вероятностью $(1-p)$ \\
То: \\
0. $E(X_{i})=p$, $Var(X_{i})=p(1-p)$,
$\hat{v}^{2}=\hat{p}(1-\hat{p})$, вместо $\bar{X}$ используем
$\hat{p}$  \\
1. $Z=\frac{\hat{p}-p}{\sqrt{\frac{p(1-p)}{n}}}$
имеет
 асимптотически  (т.е. при $n\to\infty$) нормальное распределение $N(0;1)$ \\
2. $Z=\frac{\hat{p}-p}{\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}}$ имеет
 асимптотически  (т.е. при $n\to\infty$) нормальное распределение $N(0;1)$ \\

\textbf{Две выборки} \\


Если: \\
1. $X_{i}$ - одинаково распределены между собой, \\
2. $E(X_{i})=\mu_{x}$, $Var(X_{i})=\sigma_{x}^{2}$, \\
3. $Y_{i}$ - одинаково распределены между собой, \\
4. $E(Y_{i})=\mu_{y}$, $Var(Y_{i})=\sigma_{y}^{2}$, \\
5. Все случайные величины $X_{i}$ и $Y_{j}$ независимы. \\
То: \\
1. $\frac{\bar{X}-\bar{Y}-(\mu_{x}-\mu_{y})} {\sqrt
    {
    \frac{\sigma_{x}^{2}}{n_{x}}+\frac{\sigma_{y}^{2}}{n_{y}}
    }
}$ имеет
 асимптотически  (т.е. при $n\to\infty$) нормальное распределение $N(0;1)$ \\
2. $\frac{\bar{X}-\bar{Y}-(\mu_{x}-\mu_{y})} {\sqrt
    {
    \frac{\hat{\sigma}_{x}^{2}}{n_{x}}+\frac{\hat{\sigma}_{y}^{2}}{n_{y}}
    }
}$ имеет
 асимптотически  (т.е. при $n\to\infty$) нормальное распределение $N(0;1)$ \\
3. $\frac{\bar{X}-\bar{Y}-(\mu_{x}-\mu_{y})} { \sqrt{
    \frac{(n_{x}-1)\hat{\sigma}_{x}^{2}+(n_{y}-1)\hat{\sigma}_{y}^{2}}{n_{x}+n_{y}-2}
    \cdot
    \left(\frac{1}{n_{x}}+\frac{1}{n_{y}}\right)
    }
}$ имеет асимптотически  (т.е. при $n\to\infty$) нормальное распределение $N(0;1)$ \\



Если: \\
1. $X_{i}$ - одинаково распределены между собой, \\
2. $E(X_{i})=\mu_{x}$, $Var(X_{i})=\sigma_{x}^{2}$, \\
3. $Y_{i}$ - одинаково распределены между собой, \\
4. $E(Y_{i})=\mu_{y}$, $Var(Y_{i})=\sigma_{y}^{2}$, \\
5. Все случайные величины $X_{i}$ и $Y_{j}$ независимы. \\
6. Все случайные величины $X_{i}$ и $Y_{j}$ нормально распределены \\
То: \\
1. $t=\frac{\bar{X}-\bar{Y}-(\mu_{x}-\mu_{y})} { \sqrt{
    \frac{(n_{x}-1)\hat{\sigma}_{x}^{2}+(n_{y}-1)\hat{\sigma}_{y}^{2}}{n_{x}+n_{y}-2}
    \cdot
    \left(\frac{1}{n_{x}}+\frac{1}{n_{y}}\right)
    }
}$ имеет $t$-распределение c $(n_{x}+n_{y}-2)$ степенью свободы \\
2.
$F=\frac{\hat{\sigma}_{x}^{2}/\sigma_{x}^{2}}{\hat{\sigma}_{y}^{2}/\sigma_{y}^{2}}$
имеет $F$ распределение c $(n_{x}-1)$ и $(n_{y}-1)$ степенями
свободы. \\


\textbf{Summary} \\
Для проверки гипотез о среднем:\\
Если точный закон распределения $X_{i}$ неизвестен или доподлинно
известно, что он отличается от нормального (случай 0-1), то
 используется асимптотически нормальное распределение. Т.е. требуется
 большое $n$. \\
Если известно, что $X_{i}$ нормальны, то (при любых $n$): \\
а) если известна оценка дисперсии $\hat{\sigma}^{2}$ - используем
$t$. При больших $n$ $t$-распределение перестанет отличаться от нормального. \\
б) если известна точная дисперсия (что бывает редко) - используем
нормальное распределение \\
в) можно проверять гипотезы о дисперсии с помощью $\chi^{2}$ \\



\textbf{Про} $\chi^{2}$ \textbf{распределение} \\

Проверка гипотезы о соответствии наблюдаемых частот заданному
закону распределения \\

Предполагаемые частоты: $p_{i}$ \\
Выборочные частоты: $\hat{p}_{i}$ \\

Статистика (имеет $\chi^{2}$ распределение с $(c-1)$ степенями
свободы):
$K=\sum \frac{(n\hat{p}_{i}-np_{i})^{2}}{np_{i}}$ \\
Можно, конечно, считать по иному: \\
$K=n \sum \frac{(\hat{p}_{i}-p_{i})^{2}}{p_{i}}$ \\
Или, если напрячь немного арифметический мускул: \\
$K=n \left(\sum{\frac{\hat{p}_{i}^{2}}{p_{i}}} - 1 \right)$ \\


Проверка гипотезы о независимости двух признаков \\

Выборочные частоты: $\hat{p}_{ij}$ \\
Частоты, рассчитанные в предположении независимости: $p_{ij}$ \\

Способ расчета эталонных <<независимых>> частот: \\
Сначала считаем оценки вероятностей по каждому признаку: \\
$p_{i\cdot}=\sum_{j}\hat{p}_{ij}$ \\
$p_{\cdot j}=\sum_{i}\hat{p}_{ij}$ \\
Если события $A$ и $B$ независимы, то $P(A\cap B)=P(A)P(B)$:\\
$p_{ij}=p_{\cdot j}p_{i\cdot}$ \\

Статистика (имеет $\chi^{2}$ распределение с $(r-1)(c-1)$
степенями свободы):
$K=\sum \frac{(n\hat{p}_{ij}-np_{ij})^{2}}{np_{ij}}$ \\
Или:
$K=n \sum \frac{(\hat{p}_{ij}-p_{ij})^{2}}{p_{ij}}$ \\
Или:
$K=n \left(\sum{\frac{\hat{p}_{ij}^{2}}{p_{ij}}} - 1 \right)$ \\




- Сколько степеней свободы? \\
- Столько, сколько вероятностей можно расставить <<от фонаря>> при соблюдении ограничений\\
В первом случае: \\
В обеих табличках (для $p_{i}$ и $\hat{p}_{i}$) имеется $c$ ячеек.
\\
Действует единственное общее ограничение $\sum p_{i}=1$ (и $\sum
\hat{p}_{i}=1$) \\
Значит, $(c-1)$ вероятность может быть любой, а последняя
считается исходя из того, что сумма вероятностей равна 1. \\

Во втором случае: \\
В обеих табличках (для $p_{ij}$ и $\hat{p}_{ij}$) имеется $r\cdot
c$ ячеек \\
Общие ограничения: сумма вероятностей по каждой строке и по
каждому столбцу должна быть одинакова для
$p_{ij}$ и для $\hat{p}_{ij}$. \\
Значит в каждом столбце кроме последнего можно поставить <<от
фонаря>> $r-1$ число. Последний столбец рассчитается сам собой.
Следовательно, получается $(r-1)(c-1)$ степень свободы. \\

Большинство авторов использует в формуле не частоты, а количества.
Мне этот подход кажется менее удачным, потому, что нужно
объяснять, что такое <<эталонное>> количество. При этом более
туманным (imho) становится вычисление эталонного количества для
проверки гипотезы о независимости признаков. \\
Переход от вероятностей к количествам прозрачен: \\
Эталонное количество: $E_{i}=np_{i}$ \\
Выборочное количество: $V_{i}=n\hat{p}_{i}$ \\
Соответственно меняются формулы. \\



\textbf{Some proofs}


Теорема 1. \\
Пусть $X_{i}$ iid, $N(\mu;1)$. Величину
$\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})^{2}$ можно представить в виде
$\sum_{i=1}^{n-1}Z_{i}^{2}$, где $Var(Z_{i})=1$ и
$Cov(Z_{i},Z_{j})=0$ для $i \ne j$. \\

Доказательство: \\
В качестве $Z_{k}$ возьмем $\frac{\sum_{i=1}^{k}X_{i}-kX_{k+1}}{\sqrt{n(n+1)}}$ \\
То, что $Var(Z_{i})=1$ и $Cov(Z_{i},Z_{j})=0$ проверяется "в лоб".
\\
Остается убедится в том, что $\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})^{2}=\sum_{i=1}^{n-1}Z_{i}^{2}$ \\
Доказательство (предложила Алина Дурдыева) \\

Для $n=1$ формула верна. \\
Пусть для некоторого $n$ формула верна, т.е.
$\sum_{i}^{n}(X_{i}-\bar{X}_{n})^{2}=\sum_{i}^{n-1}Z_{i}^{2}$ \\

$\sum_{i=1}^{n+1} (X_{i}-\bar{X}_{n+1})^{2}=
\sum_{i=1}^{n+1}(X_{i}-\frac{X_{n+1}+n\bar{X}_{n}}{n+1})^{2}=
\sum_{i=1}^{n+1}(X_{i}-\bar{X}_{n}-\frac{X_{n+1}-\bar{X}_{n}}{n+1})^{2}=\\$
$=\sum_{i=1}^{n}(X_{i}-\bar{X}_{n}-\frac{X_{n+1}-\bar{X}_{n}}{n+1})^{2}+
(X_{n+1}-\bar{X}_{n}-\frac{X_{n+1}-\bar{X}_{n}}{n+1})^{2}$ \\

Обозначим $b=\frac{X_{n+1}-\bar{X}_{n}}{n+1}$. \\

$\sum_{i=1}^{n+1} (X_{i}-\bar{X}_{n+1})^{2}=
\sum_{i=1}^{n}(X_{i}-\bar{X}_{n}-b)^{2}+((n+1)b-b))^2= \\
=\left[\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})^{2}+nb^{2}-2b\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})\right]
+n^{2}b^{2}= \\
=\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})^{2}+b^{2}n(n+1)$ \\
Осталось вспомнить, что
$Z_{n}=\frac{\sum_{i=1}^{n}X_{i}-nX_{n+1}}{\sqrt{n(n+1)}}$. \\
Получаем, что $\sum_{i=1}^{n+1}
(X_{i}-\bar{X}_{n+1})^{2}=\sum_{i=1}^{n-1}Z_{i}^{2}+Z_{n}^{2}$. \\


Следствие. \\
Пусть $X_{i}$ iid, $N(\mu;\sigma^{2})$.
Из теоремы и определения нетрудно видеть, что: \\
$\frac{\sum_{i}^{n}(X_{i}-\bar{X}_{n})^{2}}{\sigma^{2}}\sim
\chi_{n-1}^{2}$ \\
Если $\hat{\sigma}^{2}=\frac{\sum(X_{i}-\bar{X})^{2}}{n-1}$, то
$\frac{(n-1)\hat{\sigma}^{2}}{\sigma^{2}}\sim
\chi_{n-1}^{2}$ \\


Теорема 2. \\
Если $X_{i}$ iid, $N(\mu_{x};\sigma^{2})$ и $Y_{i}$ iid,
$N(\mu_{y};\sigma^{2})$. \\
То: $\frac{\bar{X}-\bar{Y}-(\mu_{x}-\mu_{y})} { \sqrt{
    \frac{(n_{x}-1)\hat{\sigma}_{x}^{2}+(n_{y}-1)\hat{\sigma}_{y}^{2}}{n_{x}+n_{y}-2}
    \cdot
    \left(\frac{1}{n_{x}}+\frac{1}{n_{y}}\right)
    }
}\sim t_{n_{x}+n_{y}-2}$ \\
Доказательство: \\
Вспомним, что $\frac{\sum (X_{i}-\bar{X})^{2}}{\sigma^{2}}\sim
\chi_{n_{x}-1}^{2}$ и $\frac{\sum
(Y_{j}-\bar{Y})^{2}}{\sigma^{2}}\sim
\chi_{n_{y}-1}^{2}$. \\
Следовательно, $\frac{\sum (X_{i}-\bar{X})^{2}+\sum
(Y_{j}-\bar{Y})^{2}}{\sigma^{2}}\sim
\chi_{n_{x}+n_{y}-2}^{2}$. \\
Также известно, что $Var(\bar{X}-\bar{Y})=\sigma^{2}\cdot
(\frac{1}{n_{x}}+\frac{1}{n_{y}})$. \\
Следовательно,
$\frac{\bar{X}-\bar{Y}-(\mu_{x}-\mu_{y})}{\sqrt{\sigma^{2}\cdot
(\frac{1}{n_{x}}+\frac{1}{n_{y}})}}\sim N(0;1)$. \\
По определению, $t_{k}=\frac{N(0;1)}{\sqrt{\frac{\chi^{k}}{k}}}$
\\
Получаем, что:
$$\frac{\bar{X}-\bar{Y}-(\mu_{x}-\mu_{y})} {
\sqrt{
    \frac{(n_{x}-1)\hat{\sigma}_{x}^{2}+(n_{y}-1)\hat{\sigma}_{y}^{2}}{n_{x}+n_{y}-2}
    \cdot
    \left(\frac{1}{n_{x}}+\frac{1}{n_{y}}\right)
    }
}\sim t_{n_{x}+n_{y}-2}$$


Можно получить более простой асимптотический результат. \\
Заметим, что $\frac{\bar{X}-\bar{Y}-(\mu_{x}-\mu_{y})}
{\sqrt{Var(\bar{X}-\bar{Y})}}\sim N(0;1)$. \\
$Var(\bar{X}-\bar{Y})=Var(\bar{X})+Var(\bar{Y})
=\frac{\sigma_{x}^{2}}{n_{x}}+\frac{\sigma_{y}^{2}}{n_{y}}$. \\
Заменим настоящие (неизвестные) дисперсии, на их оценки: \\
$\hat{\sigma}_{(\bar{X}-\bar{Y})}^{2}=
\frac{\hat{\sigma}_{x}^{2}}{n_{x}}+\frac{\hat{\sigma}_{y}^{2}}{n_{y}}$. \\
Получим асимптотический результат: \\
$\frac{\bar{X}-\bar{Y}-(\mu_{x}-\mu_{y})} {\sqrt
    {
    \frac{\hat{\sigma}_{x}^{2}}{n_{x}}+\frac{\hat{\sigma}_{y}^{2}}{n_{y}}
    }
}\rightarrow N(0;1)$. \\ \\


\textbf{Про упрямство и эконометрику} \\
Я упрямо обозначаю несмещенную оценку дисперсии знаком
$\hat{\sigma}^{2}$, а не $s^{2}$, как большинство европейских
авторов. \\
Почему? \\
В курсе эконометрики решается задача нахождения $\hat{\beta}_{1}$
и $\hat{\beta}_{2}$, если \\
$Y_{i}=\beta_{1}+\beta_{2}X_{i}+u_{i}$ \\
Попутно находится оценка дисперсии $u_{i}$. Она равна
$\hat{\sigma}^{2}=\frac{RSS}{n-2}$. \\
Там оценивается два параметра, поэтому $(n-2)$. А здесь мы
оцениваем один параметр - мат. ожидание $Y_{i}$: \\
$Y_{i}=\beta_{1}+u_{i}$ \\
МНК даст $\hat{\beta}_{1}=\bar{Y}$, а оценка дисперсии будет
$\hat{\sigma}^{2}=\frac{RSS}{n-1}$



\end{document}
