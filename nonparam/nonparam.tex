\documentclass[12pt,a4paper]{amsart}

\include{title_bor}

\begin{document}
\parindent=0 pt % Отступ равен 0

Коротко и строго о непараметрических тестах. (ver 26.02.08). \\
Текст можно скачать на www.xion.ru (учеба-2 курс-теория
вероятностей). \\
Вопросы/комментарии/предложения можно смело отправлять на
roah@yandex.ru (Борису Демешеву) \\

\textbf{Sign test}. Тест знаков. \\
\emph{Предпосылки}: \\
$X_{i}$ - независимы и имеют общую медиану $m$ \\
\emph{Проверяемая гипотеза}: \\
$H_{0}$: $m=m_{0}$, "медиана равна $m_{0}$" \\
$H_{a}$: $m>m_{0}$ или $m\neq m_{0}$ или $m<m_{0}$ \\
\emph{Формула расчета}: \\
$B$ - количество наблюдений больше предполагаемой медианы $m_{0}$, число <<плюсов>>. \\
\emph{Распределение при верной $H_{0}$}: \\
Биномиальное (асимптотически нормальное)\\
$E(B)=\frac{n}{2}$, $Var(B)=\frac{n}{4}$ \\


\emph{Пример}. \\
Имеются наблюдения за говорливостью 30 попугаев (слов/день): \\
34, 56, 32, 45, 34, 45, 67, 1, 34, 12, 123, ... , 37 (всего 13 наблюдений меньше 40) \\
Проверить гипотезу о том, что медиана равна 40 (слов/день). \\
\emph{Решение}. \\
Будем проверять $H_{0}$: $m=40$ против $H_{a}$: $m\neq 40$ при $\alpha=0.05$ \\
$B=17$. Поскольку число наблюдений велико, будем использовать
нормальную аппроксимацию.
$Z=\frac{B-\frac{n}{2}}{\sqrt{\frac{n}{4}}}$, где $n=30$ - число
наблюдений. \\
Получаем, что $Z=0,73$ и $Z_{critical}=1.96$. Вывод: $H_{0}$ не
отвергается, т.е. имеющиеся данные не противоречат гипотезе о том,
что половина попугаев говорит более 40 слов в день, а половина -
меньше. \\


\textbf{Wilcoxon Rank Sum Test}. Он же \textbf{Mann-Whitney test}. \\
\emph{Предпосылки}: \\
$X_{i}$, $i=1..n_{1}$ - независимы, имеют одинаковую функцию плотности $p_{X}(t)$. \\
$Y_{j}$, $j=1..n_{2}$ - независимы, имеют одинаковую функцию плотности $p_{Y}(t)$. \\
Функции $p_{X}(t)$ и $p_{Y}(t)$ имеют одинаковую форму и
отличаются только сдвигом влево-вправо, т.е. $p_{Y}(t+m)=p_{X}(t)$ \\
\emph{Проверяемая гипотеза}: \\
$H_{0}$: $m=0$, функции плотности полностью совпадают, $X$ и $Y$ взяты из одной генеральной совокупности. \\
$H_{a}$: $m>0$ или $m\neq 0$ или $m<0$ \\ \\
\emph{Иллюстрация}: \\

\begin{figure}[h]
    \includegraphics{pro_root.1}
\end{figure}


\emph{Формула расчета}: \\
Наблюдения за $X_{i}$ и $Y_{i}$ сваливаем в одну кучу и группируем
по возрастанию. Расставляем ранги (порядковые номера по возрастанию). \\
$W_{1}$ - сумма рангов всех $X_{i}$. \\
$U_{1}=W_{1}-\frac{n_{1}(n_{1}+1)}{2}$ \\
\emph{Распределение при верной $H_{0}$}: \\
Особое, есть специальные таблицы. (асимптотически нормальное)\\
$E(W_{1})=\frac{n_{1}(n_{1}+n_{2}+1)}{2}$, $Var(W_{1})=\frac{n_{1}n_{2}(n_{1}+n_{2}+1)}{12}$ \\

\emph{Формула расчета (альтернативная)}: \\
Переберем все возможные пары $(X_{i},Y_{j})$. И посчитаем, сколько раз оказалось, что $X_{i}>Y_{j}$. Обозначим это число $U_{1}$. \\
Можно например, в прямоугольной таблице (где по столбцам $X_{i}$, а по строкам $Y_{j}$) расставить <<плюсики>> и посчитать их количество. \\
Два способа расчета связаны соотношением: $U_{1}=W_{1}-\frac{n_{1}(n_{1}+1)}{2}$ \\
\emph{Распределение при верной $H_{0}$}: \\
Особое, есть специальные таблицы. (асимптотически нормальное)\\
$E(U_{1})=\frac{n_{1}n_{2}}{2}$, $Var(U_{1})=\frac{n_{1}n_{2}(n_{1}+n_{2}+1)}{12}$ \\



\emph{Доказательство формул мат. ожидания и дисперсии} \\
Следует отметить, что если $H_{0}$ верна, то $W_{1}$ - это сумма
наугад выбранных $n_{1}$ чисел из набора
$1$, $2$, $3$, ..., $(n_{1}+n_{2})$ \\
Общее количество наблюдений $N=n_{1}+n_{2}$ \\
Пусть $W_{1}$ - это сумма наугад выбранных $n_{1}$ чисел из набора
$1$, $2$, $3$, ..., $N$, а $X_{i}$ - это $i$-ое
выбранное число, т.е. $W_{1}=X_{1}+...+X_{n_{1}}$. \\
$E(W_{1})=n_{1}\cdot E(X_{1})=n_{1}\cdot \left(1\cdot \frac{1}{N}
+ ... + N\cdot \frac{1}{N} \right)=n_{1}\cdot\frac{1+N}{2}$
\\
И $E(U_{1})=E(W_{1})-\frac{n_{1}(n_{1}+1)}{2}=\frac{n_{1}n_{2}}{2}$ \\

$Var(U_{1})=Var(W_{1})$. \\
Воспользовавшись известной формулой подправки на конечность
совокупности (см. Лемму 1 из Приложения), получаем: \\
$Var(W_{1})=n_{1}\cdot Var(X_{1}) \cdot \frac{N-n_{1}}{N-1}$, где
$N=n_{1}+n_{2}$. \\
$Var(X_{1})=E(X_{1}^{2})-(E(X_{1}))^{2}$. \\
$E(X_{1}^{2})=\frac{1}{N}\cdot
(1^{2}+2^{2}+...+N^{2})$ \\
По формуле для суммы квадратов (см. Лемму 2 из Приложения),
получаем: \\
$E(X_{1}^{2})=\frac{(N+1)(2N+1)}{6}$ \\
$Var(X_{1})=\frac{(N+1)(2N+1)}{6} -
(\frac{1+N}{2})^{2}=\frac{(N+1)(N-1)}{12}$ \\
И в результате получаем, что: \\
$Var(W_{1})=\frac{n_{1}n_{2}(n_{1}+n_{2}+1)}{12}$ \\

\emph{Пример}: \\
Вашему вниманию представлены результаты прыжков в длину Васи
Сидорова. Среди болельщиц присутствовала Аня Иванова (его первая
любовь): 1,83; 1,64; 2,27; 1,78; 1,89; 2,33; 1,61; 2,31. Аня
Иванова среди болельщиц не присутствовала: 1,26; 1,41; 2,05; 1,07;
1,59; 1,96; 1,29; 1,52; 1,18; 1,47. \\
С помощью теста (Mann-Whitney) проверьте гипотезу о том, что
присутствие Ани Ивановой положительно влияет на результаты Васи
Сидорова. Уровень значимости $\alpha=0.05$. \\
\emph{Решение}: \\
$n_{1}=8$, $n_{2}=10$. \\
Формулировка гипотез: \\
$H_{0}$: $m=0$, Анино присутствие не сказывается на Васиных успехах \\
$H_{a}$: $m>0$, Аня оказывает положительное воздействие \\
После упорядочивания получаем, что результаты прыжков, при которых
Аня присутствовала, занимают 9, 10, 11, 12, 13, 16, 17 и 18 места.
Соответственно, $W_{1}=106$ и $U_{1}=70$. \\
При верной $H_{0}$, получаем, что $E(U_{1})=40$ и
$Var(U_{1})=126\frac{2}{3}$. \\
Для простоты будем пользоваться нормальной аппроксимацией. \\
Наблюдаемое $Z=2.7$ при $Z_{critical}=1.65$ (т.к. односторонняя
область). \\
Вывод: $H_{0}$ отвергается в пользу $H_{a}$, т.е. присутствие Ани
положительно сказывается на результатах Васиных прыжков в длину.
\\


\textbf{Wilcoxon Signed Rank Test}. \\
\emph{Предпосылки}: \\
Имеются парные наблюдения $X_{i}$ и $Y_{i}$, где $i=1..n$. \\
Обозначим $d_{i}=\left(X_{i}-Y_{i}\right)$. $d_{i}$ - независимы,
функции плотности $d_{i}$ симметричны относительно общего $m$.
Сами функции плотности могут не совпадать. Среднее у разных
$X_{i}$ может быть разным, сами $X_{i}$ могут быть зависимыми. \\
\emph{Проверяемая гипотеза}: \\
$H_{0}$: $m=0$, переход от $X_{i}$ к $Y_{i}$ не меняет среднего \\
$H_{a}$: $m>0$ или $m\neq 0$ или $m<0$ \\ \\
Примечание: \\
Возможен иной вариант применения этого теста: \\
\emph{Вариант предпосылок}: \\
$X_{i}$ - независимы и симметрично распределены относительно
общего среднего $m$. В остальном законы распределения могут
отличаться. В этом случае, $d_{i}=X_{i}-m_{0}$ \\
\emph{Вариант проверяемой гипотезы}: \\
$H_{0}$: $m=m_{0}$, "медиана (или среднее) равна $m_{0}$" \\
$H_{a}$: $m>m_{0}$ или $m\neq m_{0}$ или $m<m_{0}$ \\
\emph{Формула расчета}: \\
Упорядочиваем $d_{i}$ по возрастанию абсолютной величины.
Расставляем ранги (порядковые номера по возрастанию). \\
$T^{+}$ - сумма рангов положительных $d_{i}$. \\
\emph{Распределение при верной $H_{0}$}: \\
Особое, есть специальные таблицы. (асимптотически нормальное)\\
$E(T^{+})=\frac{n(n+1)}{4}$ и $Var(T^{+})=\frac{n(n+1)(2n+1)}{24}$\\



\emph{Доказательство формул мат. ожидания и дисперсии}: \\
Можно представить $T^{+}$ в виде: \\
$T^{+}=1\cdot I_{1}+2\cdot I_{2}+...+n\cdot I_{n}$, где $I_{k}$
равно $1$, если ранг $k$ достался положительному $d_{i}$. Если
$H_{0}$ верна, то $P(I_{k}=1)=0.5$.\\
Следовательно: \\
$E(T^{+})=\frac{1}{2}\cdot(1+2+...+n)=\frac{n(n+1)}{4}$ \\
Для дисперсии снова воспользуемся формулой для суммы квадратов
(лемма 2 из Приложения):
$Var(T^{+})=\frac{1}{4}\cdot(1^{2}+2^{2}+...+n^{2})=\frac{n(n+1)(2n+1)}{24}$ \\

\emph{Пример}: \\
Некоторые результаты 2-х контрольных по теории вероятностей
выглядят следующим образом (указан результат за вторую
контрольную и в скобках результат за первую): \\
43(55), 113(108), 97(53), 68(42), 94(67), 90.5(97), 35(91),
126(127), 102(78), 89(83). \\
Можно ли считать (при $\alpha=0.05$), что вторую контрольную
написали лучше? \\
\emph{Решение}: \\
Предположим, что изменения результатов имеют симметричные
распределения относительно общего числа $m$. \\
Проверяем: \\
$H_{0}$: $m=0$, в среднем результат такой же \\
$H_{a}$: $m>0$, вторую контрольную написали лучше \\
Разницы $d_{i}$ равны (упорядочены по модулю): -1, 5, 6, -6.5, -12, 24, 26, 27, 44, -56 \\
Положительные $d_{i}$ занимают места: 2, 3, 6, 7, 8, 9, значит
$T^{+}=35$. \\
Если $H_{0}$ верна, то $E(T^{+})=27.5$ и $Var(T^{+})=96.25$
($n=10$). \\
Для простоты используем нормальное распределение: \\
Получаем наблюдаемое $Z=0.76$ при $Z_{critical}=1.65$ \\
Вывод: имеющиеся наблюдения не противоречат $H_{0}$. Т.е. первую и
вторую контрольную написали в целом одинаково. \\

\textbf{Run's test}. Тест серий. \\
\emph{Предпосылки}: \\
Имеются результаты $N$ последовательных испытаний. Каждое из них
это успех ($+$) или неуспех ($-$). Имеется $n_{+}$ успехов и
$n_{-}$ неуспехов. \\
\emph{Проверяемая гипотеза}: \\
$H_{0}$: Испытания независимы. \\
$H_{a}$: Испытания зависимы, причем зависимость сказывается на
ожидаемом числе серий. (Строгая ли???) \\
\emph{Формула расчета}: \\
Считаем $T$ число серий (серия - это последовательность из
одинаковых знаков). Число серий равно числу смен знака плюс
единица. \\
\emph{Распределение при верной $H_{0}$}: \\
Особое, есть специальные таблицы. (асимптотически нормальное)\\
$E(T)=\frac{2n_{+}n_{-}}{n_{+}+n_{-}}+1$ и
$Var(T)=\frac{2n_{+}n_{-}(2n_{+}n_{-}-N)}{N^{2}(N+1)}$ \\



\emph{Доказательство формул мат. ожидания и дисперсии}: \\
Обозначим $N=n_{+}+n_{-}$. Если верна $H_{0}$, то можно
представить себе следующий эксперимент: в корзине перемешиваются
$n_{+}$ успехов и $n_{-}$ неуспехов, затем их достают из корзины по очереди. \\
Разложим $T$ в сумму: \\
$T=I_{1}+I_{2}+...+I_{N}$, где $I_{k}$ равно $1$, если на $k$-ом
числе происходит начало новой серии (смена знака). При этом
$I_{1}=1$ тождественно. \\
Далее пригодятся несколько вспомогательных результатов: \\
$E(I_{2})=\frac{n_{+}}{N}\cdot\frac{n_{-}}{N-1}+\frac{n_{-}}{N}\cdot\frac{n_{+}}{N-1}
=\frac{2n_{+}n_{-}}{N(N-1)}$ и
$E(I_{2}^{2})=\frac{2n_{+}n_{-}}{N(N-1)}$.\\
Для определения $E(I_{i}I_{j})$ важно знать расстояние между $i$ и
$j$. \\
$E(I_{2}I_{3})=\frac{n_{+}}{N}\frac{n_{-}}{N-1}\frac{n_{+}-1}{N-2}+
\frac{n_{-}}{N}\frac{n_{+}}{N-1}\frac{n_{-}-1}{N-2}=
\frac{n_{+}n_{-}}{N(N-1)}$. \\
$E(I_{2}I_{4})= P(I_{2}=1)\cdot
P(I_{4}=1|I_{2}=1)=\frac{2n_{+}n_{-}}{N(N-1)}
\frac{2(n_{+}-1)(n_{-}-1)}{(N-2)(N-3)}$ \\
Теперь легко находим: \\
$E(T)=1+(N-1)\cdot E(I_{2})$, и, следовательно, $E(T)=1+2\frac{n_{+}n_{-}}{N}$. \\
$Var(T)=Var(I_{2}+I_{3}+...+I_{N})= \\
=(N-1)Var(I_{2})+2(N-2)Cov(I_{2},I_{3})+(N-2)(N-3)Cov(I_{2},I_{4})= \\
=(N-1)E(I_{2}^{2})+2(N-2)E(I_{2}I_{3})+(N-2)(N-3)E(I_{2}I_{4})-(N-1)^2(E(I_{2}))^{2}=
$ \\
После подстановки и упражнения по алгебре 9-го класса, получаем: \\
$Var(T)=\frac{2n_{+}n_{-}(2n_{+}n_{-}-N)}{N^{2}(N+1)}$ \\

\emph{Пример}: \\
Садовник осматривал по очереди розовые кусты вдоль ограды. Всего вдоль ограды растет 30 розовых кустов. Из них оказалось 20 здоровых и 10 больных. \\
Вот заметки садовника: $+++\ominus++\ominus\ominus\ominus++++\ominus\ominus ++\ominus+++++\ominus\ominus\ominus++++$ \\
(+ - здоровый куст, $\ominus$ - больной куст) \\
а) С помощью теста серий проверьте гипотезу о независимости испытаний \\
б) Какой естественный смысл имеет эта гипотеза? \\
Подсказка: можно использовать нормальное распределение \\
\emph{Решение}: \\
Запишем условие в наших обозначениях: $T=11$, $n_{+}=20$, $n_{-}=10$, $N=30$ \\
Следовательно, $E(T)=14.33$, $Var(T)=5.30$ \\
Выбираем уровень значимости 5\% \\
Какой должна быть альтернативная гипотеза? \\
В данном случае логично предположить, что альтернативной гипотезой является <<заразность>> заболевания, т.е. имеем одностороннюю область, где $H_{0}$ отвергается \\
Используем нормальное распределение. $Z=-1.45$, $Z_{critical}=-1.64$ \\
Вывод $H_{0}$ - не отвергается. \\

Приложение. \\

Лемма 1. Подправка на конечность совокупности. \\
Пусть имеется $N$ чисел, из которых наугад выбираются $n$. \\
Обозначим $X_{i}$ - $i$-ое извлекаемое число, $S_{n}$ - сумму
извлеченных чисел, $\overline{X}_{n}$ - среднее арифметическое
извлеченных чисел и $\sigma^{2}=Var(X_{i})$. \\
В таких обозначениях: \\
$Cov(X_{i},X_{j})=-\frac{\sigma^{2}}{N-1}$ при $i\ne j$. \\
$Var(\overline{X}_{n})=\frac{\sigma^{2}}{n}\cdot\frac{N-n}{N-1}$.
\\
$Var(S_{n})=n\cdot
\sigma^{2}\cdot\frac{N-n}{N-1}$. \\
Доказательство: \\
Заметим, что $\sum_{i=1}^{N} X_{i}=const$, поэтому
$Cov(X_{1},X_{1}+X_{2}+...+X_{N})=0$. \\
Воспользуемся тем, что
$Cov(X_{1},X_{2})=Cov(X_{1},X_{3})=...=Cov(X_{1},X_{N})$. \\
Таким образом мы получаем $Var(X_{1})+(N-1)Cov(X_{1},X_{2})=0$. \\
И, следовательно, $Cov(X_{i},X_{j})=-\frac{\sigma^{2}}{N-1}$ для $i\ne j$. \\
Значит $Var(\frac{1}{n}\sum_{i=1}^{n} X_{i})= \frac{1}{n^{2}}
\left( n \cdot \sigma^{2}+2 \cdot C_{n}^{2} \cdot \left(
-\frac{\sigma^{2}}{N-1} \right)
\right)=\frac{\sigma^{2}}{n}\cdot\frac{N-n}{N-1}$ \\
Осталось заметить, что  $Var(S_{n})=n\cdot
\sigma^{2}\cdot\frac{N-n}{N-1}$ \\


Лемма 2. \\
$$ \sum_{i=1}^{n} i^{2} = \frac{n(n+1)(2n+1)}{6} $$
Доказательство: \\
Для $n=1$ формула верна. \\
Пусть она верна для некоторого $n$. Докажем ее для $(n+1)$. \\
$$\sum_{i=1}^{n+1} i^{2} =
\frac{n(n+1)(2n+1)}{6}+(n+1)^{2}=...=\frac{[n+1]([n+1]+1)(2[n+1]+1)}{6}$$\\
%С одной стороны:
%$$ \sum_{i=1}^{n+1} i^{3} = \sum_{i=1}^{n} i^{3} + (n+1)^{3}$$
%С другой стороны:
%$$ \sum_{i=1}^{n+1} i^{3} = \sum_{i=1}^{n+1} (i-1)^{3} + \sum_{i=1}^{n+1}
%1^{3}+ \sum_{i=1}^{n+1} 3\cdot (i-1)\cdot 1^{2} + \sum_{i=1}^{n+1}
%3\cdot(i-1)^{2}\cdot 1 $$
%$$ \sum_{i=1}^{n+1} i^{3} = \sum_{i=1}^{n} i^{3} + (n+1) + 3\cdot
%(1+2+...+n)+3\cdot (1^{2}+2^{2}+...+n^{2}) $$
%$$ \sum_{i=1}^{n+1} i^{3} = \sum_{i=1}^{n} i^{3} + (n+1) + 3\cdot
%\frac{n(n+1)}{2} + 3\cdot \sum_{i=1}^{n} i^{2}$$

%Приравняв и убрав $\sum_{i=1}^{n} i^{3}$, получаем: \\
%$$(n+1)^{3}=(n+1) + 3\cdot
%\frac{n(n+1)}{2} + 3\cdot \sum_{i=1}^{n} i^{2}$$

%Или $$\sum_{i=1}^{n} i^{2} = \frac{n(n+1)(2n+1)}{6} $$

P.S. \\
Современная педагогика зародилась в Абхазии, где высоко в горах
одаренные чабаны могли часами удерживать аудиторию в пять, а то и
в шесть тысяч баранов. \\




\end{document}
