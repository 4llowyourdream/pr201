\documentclass[12pt,a4paper]{amsart}
\usepackage{cmap}
%\usepackage{amssymb}
\usepackage{graphicx}
% для latex=>ps можно использовать [dvips]; latex=>pdf [pdftex]

\usepackage[russianb]{babel}
\usepackage[cp1251]{inputenc} % работает и без, но говорят лучше с этой строчкой

\evensidemargin=0pt
\oddsidemargin=0pt
\topmargin=0pt
\textwidth=17cm

\usepackage{ifpdf}
\ifpdf
	\DeclareGraphicsRule{*}{mps}{*}{}
\fi

\begin{document}
\parindent=0 pt % отступ равен 0
Ликбез по методу максимального правдоподобия! \\



Идея метода: \\
Имеется $n$ наблюдений, значений случайных величин. Параметр $\theta$ не известен. Считаем вероятность получить имеющиеся наблюдения. Она зависит от $\theta$. Выбираем $\hat{\theta}$ так, чтобы вероятность имеющихся наблюдений была наибольшей. \\


Пример с вероятностью. \\


Пример с функцией плотности. \\


Чем хорош метод максимального правдоподобия? \\

Теорема: \textbf{Оценки ML <<хорошие>>}: \\
Оценки будут: \\
\textbf{Состоятельными}: \\
$\lim_{n\to\infty} P(|\hat{\theta}_{n}-\theta|>\epsilon)=0$ \\
\textbf{Асимптотически несмещенными}: \\
$\lim E(\hat{\theta}_{n})=\theta$ \\
\textbf{Асимптотически нормальными}. \\
$\hat{\theta}_{n}\sim N$ \\


Информацией Фишера (информационной матрицей) называют $I=-E(l''(\theta))$ \\
Теоремка: $I=-E(l''(\theta))=E(l'(\theta)\cdot l'(\theta))$ \\




Теорема: \textbf{Неравенства Крамера-Рао} 
У любой другой несмещенной оценки дисперсия не меньше, чем $I^{-1}$ \\


Теорема: \textbf{Оценки ML <<лучшие>>}: \\
Оценки ML являются асимптотически эффективными, т.е. их дисперсия является асимптотически минимально возможной и равна $I^{-1}$ \\
\textbf{Асимптотически эффективными} среди асимпотически нормальных несмещенных. \\
Если взять другую асимпотически несмещенную нормальную оценку $\hat{\theta}_{n}^{alt}$, то у нее будет выше дисперсия, $Var(\hat{\theta}_{n}^{alt})\ge Var(\hat{\theta}_{n}^{ML})$ \\


Подведем итог: $\hat{\theta}\sim N(\theta,I^{-1})$ \\

Настояющая $I$ зависит от неизвестного $\theta$ \\
Однако можно оценить $\hat{I}$ по принципу: $\hat{I}=-l''(\hat{\theta})$ \\
Значит $\widehat{Var}(\hat{\theta})=\hat{I}^{-1}$ \\




С помощью ML можно проверять гипотезы:  \\
$H_{0}:\theta=\theta_{0}$
Святая троица ML тестов \\
Тест Вальда (Wald test): \\
$W=(\hat{\theta}-\theta_{0})\cdot I\cdot (\hat{\theta}-\theta_{0})$ \\

Score test (Lagrange multiplier test): \\
$LM=l'(\theta_{0})\cdot I^{-1}\cdot l'(\theta_{0})$ \\

Likelihood ratio test: \\
$LR=-2(l(\theta_{0})-l(\hat{\theta}))$ \\

Если нулевая гипотеза верна, то все три статистики имеют $\chi^{2}$ распределение с 1-ой степенью свободы. \\

Комментарий: формулы записаны в таком странном виде, чтобы были похожи на многомерный случай. \\


Прочие факты: \\

Факт 1. $E(l(\theta))$ достигает своего максимума при истинном $\theta$ \\

Факт 2. $Var(l'(\theta))=I(\theta)$ \\




Многомерный случай...\\
Имеет $k$ параметров $\theta=(\theta_{1}$,... $\theta_{k})^{t}$ \\
Проверяется гипотезы, состоящая из $j$ ограничений. \\
В этом случае произойдут такие изменения: \\
Вместо производной $l'(\theta)$ будет вектор градиент $grad(l)$ \\
Вместо второй производной $l''(\theta)$ будет матрица Гессе $H(\theta)$ \\
Вместо $I$ будет матрица \\
Оценка $\hat{I}$ считается как $\hat{I}=-H(\hat{\theta})$ \\
Теоремка примет вид: $I=-E(H(\theta))=E(grad(l)^{t}grad(l))$ \\
Изменятся формулы трех статистик. \\

Статистики будут иметь $\chi^{2}_{j}$ распределение \\


Примечания мелким шрифтом: \\
Указанные теоремы верны при соблюдении следующих технических условий:\\




Более сильный вариант неравенства Крамера-Рао имеет вид: \\




Доказательства можно найти, например, в\\

\end{document}