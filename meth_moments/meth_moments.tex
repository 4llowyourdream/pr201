
\section{Метод моментов}


Пусть случайные величины $X_{1}$, ..., $X_{n}$ независимы и одинаково распределены. Закон больших чисел говорит нам, что среднее выборочное $ \bar{X} $ является хорошей оценкой для математического ожидания $ \E(X_{i}) $:

\[ \bar{X}_{n} \stackrel{P}{\longrightarrow} \E(X_{i}) \]

На практике это означает, что при больших $ n $ эти величины равны:

\[ \bar{X}_{n}\approx \E(X_{i}) \]

На этой нехитрой идее и построен метод моментов. Как конкретно используется идея понятно из следующих двух примеров:

Пример 1. Допустим, что случайные величины $ X_{1} $, ..., $ X_{n} $ независимы и равномерны на $ [\theta;\theta+1] $. Постройте оценку неизвестного параметра $ \theta $ с помощью метода моментов.

В данном случае $ \E(X_{i})=\theta+0.5 $ и, следовательно:

\[ \bar{X}_{n}\approx \theta+0.5 \]
Выражаем $ \theta $:
\[ \theta\approx \bar{X}_{n}-0.5 \]
Это и есть нужная нам оценка:
\[ \hat{\theta}_{MM}:=\bar{X}_{n}-0.5 \]


Пример 2. Неправильная монетка выпадает орлом с неизвестной вероятностью $ p $. Провели несколько экспериментов и каждый раз записывали, сколько раз ее потребовалось подкинуть до появления первого орла. Обозначим эти величины $X_{1}$, ..., $ X_{n} $. Постройте оценку неизвестного параметра $ p $ с помощью метода моментов.

Величины $ X_{i} $ имеют геометрическое распределение, поэтому $ \E(X_{i})=\frac{1}{p} $. Принцип метода моментов гласит:

\[ \bar{X}_{n}\approx \frac{1}{p}\]
Выражаем неизвестный параметр $ p $:
\[ p\approx \frac{1}{\bar{X}_{n}} \]
Это и есть нужная нам оценка:
\[ \hat{p}_{MM}:= \frac{1}{\bar{X}_{n}} \]


Если говорить более формально...

Определение. Пусть $ X_{i} $ одинаково распределены и независимы, а $ \E(X_{i}) $ зависит от неизвестного параметра $ \theta $, скажем $ \E(X_{i})=f(\theta) $. Тогда оценкой метода моментов называется случайная величина:

\[ \hat{\theta}_{MM}:=f^{-1}(\bar{X}_{n}) \]

Конечно, иногда бывают ситуации, когда математическое ожидание $ \E(X_{i}) $ не зависит от $ \theta $. Например, если $ X_{i} $ равномерны на $ [-\theta;\theta] $, то математическое ожидание $ \E(X_{i})=0 $. Что делать в такой ситуации? 

Неспроста же наш метод называется методом моментов... Напомним, что $ k $-ым моментом случайной величины $ X_{i} $ называется математическое ожидание $ \E(X_{i}^{k}) $...


Итак, если условия $\bar{X}_{n}\approx \E(X_{i})$ связанного с первым моментом не хватило, то на помощь придет второй момент случайной величины. В силу того же закона больших чисел:

\[ \frac{\sum X_{i}^{2}}{n} \approx \E(X_{i}^{2})\]


Пример 3. Величины $ X_{i} $ независимы и равномерны на $ [-\theta;\theta] $. Постройте оценку неизвестного параметра $ \theta $ с помощью метода моментов.

Убеждаемся, что $\E(X_{i})=0$. Находим $ \E(X_{i}^{2}) $:

\[  \E(X_{i}^{2}) = \int _{-\theta}^{\theta} x^{2} \frac{1}{2\theta}dx=...=\frac{\theta^{2}}{3} \]

Согласно принципу метода моментов:

\[ \frac{\sum X_{i}^{2}}{n} \approx \frac{\theta^{2}}{3} \]

Выражаем $ \theta $:

\[ \theta\approx \sqrt{3\frac{\sum X_{i}^{2}}{n} }\]

Это и есть нужная нам оценка:

\[ \hat{\theta}_{MM}= \sqrt{3\frac{\sum X_{i}^{2}}{n} }\]

Если не хватит и второго момента, тогда воспользуемся третьим и т.д. Для произвольного $k$ мы имеем:

\[ \frac{\sum X_{i}^{k}}{n} \approx \E(X_{i}^{k})\]

В большинстве случаев хватает именного первого момента. Последующие моменты нужны чаще всего при оценке нескольких параметров.







